{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 16A Shazam\n",
    "### EECS 16A: Designing Information Devices and Systems I, Fall 2024\n",
    "\n",
    "Taken and modified from EE 120: Signals and Systems at UC Berkeley\n",
    "\n",
    "Acknowledgements:\n",
    "\n",
    "- **Spring 2020** (v1.0): Anmol Parande, Dominic Carrano, Babak Ayazifar\n",
    "- **Spring 2022** (v2.0): Anmol Parande\n",
    "- **Spring 2023** (v2.1): Yousef Helal\n",
    "- **Fall 2023** (v2.2): Christine Zhang\n",
    "- **Fall 2024** (v3.0): Nikhil Ograin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In 2002, Shazam Entertainment Limited (founded by UC Berkeley students!) launched its music identification product, allowing users to dial a phone number and play a song. Then, they'd get a text message with the name of the song and its artist. In 2018, Shazam was acquired by Apple for \\$400 million, and it's now in every iPhone.\n",
    "\n",
    "Shazam works by using *audio fingerprinting*: given a song, it generates a set of identifiers, and searches an audio database to find a match and identify the song. In this lab, you'll learn about audio fingerprinting, and use it to build a music identification just like Shazam!\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "To get started, you'll need to install [Pandas](https://pandas.pydata.org/docs/getting_started/install.html). And, of course, you'll also need NumPy, SciPy, and Matplotlib if you don't already have them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy.ndimage import maximum_filter\n",
    "from shazam_utils import generate_hash\n",
    "\n",
    "import autograder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: To avoid any copyright issues, we've cropped all provided songs to only contain the first 60 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Spectral Analysis\n",
    "\n",
    "As we've already seen throughout the course, a signal's constituent frequencies tell us a lot about it. The same is true of audio: to find the salient features of songs to fingerprint, we'll need to look at the song's spectrum (i.e., Fourier Transform). Fortunately, we have the DFT (efficiently implemented via the FFT) to help us do this.\n",
    "\n",
    "To get started, let's load in *Viva La Vida* by Coldplay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, we think of audio as a two-channel, continuous signal $\\vec{x}(t) = \\left[x_L(t) \\ x_R(t)\\right]$, with one column of the *audio matrix* per channel. That is, $x_L(t)$ is the left channel's signal, and $x_R(t)$ the right channel's signal. The reason we have two distinct audio channels is so that we can have two streams playing at the same time, one per ear (e.g., in a pair of headphones or laptop speakers).\n",
    "\n",
    "We sample this CT audio signal at a particular rate (here, 48000 Hz) to get a DT signal. For our purposes, the distinction between our channels is not very important, so we'll just average them to form a 1D signal, $x(n)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "print(\"Audio Shape: {0}, Sampling Rate: {1} Hz\".format(coldplay.shape, fs))\n",
    "coldplay = np.mean(coldplay, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense for the song we're working with, feel free to have a listen! This cell may take a few seconds to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"VivaLaVida.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1a: One DFT is Not Enough\n",
    "\n",
    "As far as spectral analysis is concerned, it seems like we should just be able to take the DFT of the entire song, find our fingerprints, and be done, right? Is that really all there is to Shazam? No, not quite. It may not be obvious, but there's a big issue with this approach that we'll explore now. So that our code doesn't take forever to run, we'll only look at the first 10 seconds of the song, but the issues we'll find here apply generally to the entire signal.\n",
    "\n",
    "To start, let's define a function which will give us the magnitude spectrum of the signal $|X(\\omega)|$ centered around $\\omega = 0$. By default, when you compute the FFT, the samples of the DTFT that are returned go from $0$ to $2\\pi$; centering them so that they go from $-\\pi$ to $\\pi$ is nicer for visualization.\n",
    "\n",
    "Fill in the code for `centered_magnitude_spectrum`, which takes in a signal and outputs its centered magnitude spectrum.\n",
    "\n",
    "**Hint**: Check out [np.fft.fftshift](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fftshift.html) to center your spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centered_magnitude_spectrum(sig):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    sig - a generic iterable signal of floating point numbers\n",
    "    \n",
    "    Output:\n",
    "    Returns a centered magnitude spectrum of the given signal. \n",
    "    That is, the DTFT of the signal after shifting from [0,2pi] to [-pi,pi].\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograder.test_Q1a(centered_magnitude_spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why one DFT won't suffice, we're going to look at the spectrum of different sections of Viva La Vida.\n",
    "\n",
    "First, we'll look at magnitude spectrum of the first 10 seconds of the song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldplay_cropped = coldplay[:10 * fs]\n",
    "coldplay_freqs = centered_magnitude_spectrum(coldplay_cropped)\n",
    "\n",
    "plt.figure(figsize=(16, 4), dpi=200)\n",
    "freqs = np.linspace(-fs/2, fs/2, len(coldplay_freqs))\n",
    "plt.plot(freqs, coldplay_freqs)\n",
    "plt.xlabel(\"Frequency [Hz]\")\n",
    "plt.ylabel(\"Magnitude\")\n",
    "plt.title(\"DFT of first 10 seconds of Viva La Vida\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the frequency content is centered around the lower frequencies. In fact, we can barely see anything past 10 kHz, because human hearing stops around 15-20 kHz (and generally decreases with age), so there's no reason to include anything that high in music."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, everything looks ok: we got the spectrum of the first 10 seconds of our song. This gives us a sort of \"aggregate view\" of the frequencies that show up at some point during the first 10 seconds. But is this \"aggregate view\" good enough? What happens if our signal is *non-stationary*, i.e its frequency content changes with time, as is certainly the case with music? \n",
    "\n",
    "To find out, let's look at the magnitude spectra of the first, second, third, and fourth seconds of the song. We'll use these to zoom in (temporally speaking) and inspect the song's frequency content over the course of a second of data (rather than 10), and see if the \"aggregate view\" gives a good enough picture of what frequencies are present at a specific second in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldplay_freqs_1 = centered_magnitude_spectrum(coldplay[:fs]) \n",
    "coldplay_freqs_2 = centered_magnitude_spectrum(coldplay[fs:2*fs])\n",
    "coldplay_freqs_3 = centered_magnitude_spectrum(coldplay[2*fs:3*fs])\n",
    "coldplay_freqs_4 = centered_magnitude_spectrum(coldplay[3*fs:4*fs])\n",
    "\n",
    "freqs = np.linspace(-fs / 2, fs / 2, len(coldplay_freqs_1))\n",
    "sigs = [coldplay_freqs_1, coldplay_freqs_2, coldplay_freqs_3, coldplay_freqs_4]\n",
    "strs = [\"1st\", \"2nd\", \"3rd\", \"4th\"]\n",
    "\n",
    "plt.figure(figsize=(16, 10), dpi=200)\n",
    "for i in range(1, 5):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(freqs, sigs[i-1])\n",
    "    plt.xlim([-.5e4, .5e4])\n",
    "    plt.ylim([0, 1.1 * np.array(sigs).max()])\n",
    "    plt.title(\"DFT magnitude of {} second of Viva La Vida\".format(strs[i-1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how while most of the energy in each second's spectrum is concentrated inside $[-2.5 \\text{ kHz}, +2.5 \\text{ kHz}]$, the exact shapes are quite different. \n",
    "\n",
    "**The issue is that the aggregate view from our 10-second DFT doesn't have good enough *temporal resolution*: we can't see how the signal's frequency content changes over time!**\n",
    "\n",
    "Why does this matter, you ask? Well, when we're working with the real deal, we don't feed Shazam the entire song; only a clip. For example, suppose you tune into a radio station halfway through a song. Then, 20 seconds later, you think to yourself, \"hey, I like this\" and pull out Shazam to figure out what song it is. By then, whatever you're giving Shazam is missing a lot of data, and so it needs to be able to look at what frequencies are in the song at different points in time to correctly identify it. The aggregate view won't do. Fortunately, there's a very simple fix to this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1b: Spectrogrammin'\n",
    "\n",
    "The results of Q1a are pretty clear: we need a way to see how the signal's frequency content changes over time. Just taking one DFT of the entire signal fails to achieve this. Instead, we'll use a *spectrogram*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrograms\n",
    "\n",
    "A *spectrogram* is an image representing the frequency content of a signal at different times. This ability to see how a signal's frequency content changes with time is the key useful feature of a spectrogram. \n",
    "\n",
    "To compute a spectrogram, we split our signal into chunks, compute the DFT of each chunk, and plot the magnitude squared of those DFT chunks side-by-side. To make visualization easier, we typically employ a colormap to distinguish where the DFT's squared-magnitude is bigger.\n",
    "\n",
    "For example, here is a spectrogram of speech, taken from [here](https://www.researchgate.net/figure/Spectrogram-of-a-speech-signal-with-breath-sound-marked-as-Breath-whose-bounds-are_fig1_319081627). The red areas correspond to stronger frequency content, and green areas to weaker frequency content.\n",
    "\n",
    "Notice the differences between when the speaker takes a breath and when the speaker is actually speaking. A single DFT wouldn't be able to separate this!\n",
    "\n",
    "![image.png](speech-spectrogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get some familiarity with spectrograms, let's generate some sinusoidal signals and plot their spectrograms. You don't need to write any code for this question. Just run the cells!\n",
    "\n",
    "In the cell below, we generate 1000 samples of the following signals over the interval $t \\in [0, 1]$:\n",
    "- A 100 Hz sine wave (call it `x1`).\n",
    "- A 400 Hz sine wave (call it `x2`).\n",
    "- A third signal, call it `x3`, by concatenating `x1` and `x2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0, 1, 1000)\n",
    "x1 = np.sin(2 * np.pi * 100 * n)\n",
    "x2 = np.sin(2 * np.pi * 400 * n)\n",
    "x3 = np.concatenate((x1, x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the DFT of our signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_1 = centered_magnitude_spectrum(x1)\n",
    "freqs_2 = centered_magnitude_spectrum(x2)\n",
    "freqs_3 = centered_magnitude_spectrum(x3)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n",
    "\n",
    "axs[0].plot(np.linspace(-500,500, len(freqs_1)), freqs_1)\n",
    "axs[0].set_ylabel('DFT Magnitude')\n",
    "axs[0].set_xlabel('Frequency [Hz]')\n",
    "axs[0].set_title(\"100 Hz Signal\")\n",
    "\n",
    "axs[1].plot(np.linspace(-500,500, len(freqs_2)), freqs_2)\n",
    "axs[1].set_ylabel('DFT Magnitude')\n",
    "axs[1].set_xlabel('Frequency [Hz]')\n",
    "axs[1].set_title(\"400 Hz Signal\")\n",
    "\n",
    "axs[2].plot(np.linspace(-500,500, len(freqs_3)), freqs_3)\n",
    "axs[2].set_ylabel('DFT Magnitude')\n",
    "axs[2].set_xlabel('Frequency [Hz]')\n",
    "axs[2].set_title(\"100 Hz and 400 Hz Signal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the *pure tones* (the 100 Hz and 400 Hz sine waves) have 2 peaks each, due to conjugate symmetry, whereas the signal formed by concatenating them has 4 peaks. Now, let's look at the spectrograms of these signals. Run the following code to plot the spectrogram of each signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, t1, x1_freqs = signal.spectrogram(x1, fs=1000)\n",
    "f2, t2, x2_freqs = signal.spectrogram(x2, fs=1000)\n",
    "f3, t3, x3_freqs = signal.spectrogram(x3, fs=1000)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 4), dpi=200)\n",
    "\n",
    "axs[0].pcolormesh(t1, f1, x1_freqs, cmap=\"gray\", shading='auto')\n",
    "axs[0].set_ylabel('Frequency [Hz]')\n",
    "axs[0].set_xlabel('Time [sec]')\n",
    "axs[0].set_title(\"100 Hz Signal\")\n",
    "\n",
    "axs[1].pcolormesh(t2, f2, x2_freqs, cmap=\"gray\", shading='auto')\n",
    "axs[1].set_ylabel('Frequency [Hz]')\n",
    "axs[1].set_xlabel('Time [sec]')\n",
    "axs[1].set_title(\"400 Hz Signal\")\n",
    "\n",
    "axs[2].pcolormesh(t3, f3, x3_freqs, cmap=\"gray\", shading='auto')\n",
    "axs[2].set_ylabel('Frequency [Hz]')\n",
    "axs[2].set_xlabel('Time [sec]')\n",
    "axs[2].set_title(\"100 Hz and 400 Hz Signal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first spectrogram has a single band at 100 Hz. The second has a single band at 400 Hz. The final one has two bands (one at 100 Hz and one at 400 Hz). The reason we aren't seeing conjugate symmetry here is because we are only plotting the positive frequencies. For the most part, these spectrograms appear to give us the same information as the DFT. \n",
    "\n",
    "However, notice that in the 3rd spectrogram, the frequencies are mostly only present for the duration they exist. There's some overlap between 1.0-1.2 seconds, which isn't what we would have expected. This happens because SciPy doesn't truly use distinct chunks, as we mentioned above, and instead goes with a more sophisticated overlapping window approach, covered in EE 123 (this gives a better tradeoff between the temporal and spectral resolutions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1c: Spectrograms of Songs\n",
    "\n",
    "Now that we've got the basic concepts down, let's load *Viva La Vida* and *Mr. Brightside* and compare their spectrograms. Run the cell below to load the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "coldplay = np.mean(coldplay, axis=1)\n",
    "\n",
    "fs, killers = wavfile.read(\"MrBrightside.wav\")\n",
    "killers = np.mean(killers, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we haven't heard *Mr. Brightside* yet, let's load it in now and have a listen. This cell will take a few seconds to load before the audio interface shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(\"MrBrightside.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better looking image when visualizing the spectrogram, we'll plot everything in decibels. \n",
    "\n",
    "Recall that to convert a number $x$ to decibels, we compute $x_\\text{dB} = 20\\log_{10}(x).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Job\n",
    "\n",
    "In the cell below:\n",
    "1. Use [`signal.spectrogram`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html) to compute the spectrogram of each song. \n",
    "    - Use 4096 for the `nperseg` parameter of `signal.spectrogram` to take a 4096 point DFT. This matches the length of DFT typically used in practical audio fingerprinting systems, representing a good tradeoff between spectral and temporal resolution.\n",
    "    - The function returns a tuple containing the frequencies of the spectrogram samples, time points of the spectrogram samples, and the actual spectrogram. For each call to `signal.spectrogram`, store these as `fi, ti, []_spect` where `i` is 1 or 2 and `[]` gets filled with `coldplay` or `killers` depending on the song.\n",
    "2. Convert the resultant spectrograms to the decibel scale using the formula from above.\n",
    "\n",
    "**To ensure there are no divide by zero warnings and to make sure the spectrogram renders properly, please add a small positive constant of $10^{-12}$** (provided as `EPSILON_DB_CONSTANT`) **before taking the log when converting to decibels.**\n",
    "\n",
    "*Hint*: make sure you use `np.log10` in your computations\n",
    "\n",
    "*Hint*: Don't forget to pass in the sampling frequency, `fs`, into the spectrogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_DB_CONSTANT = 1e-12\n",
    "\n",
    "def compute_spectrogram(fs, audio):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    fs - the sampling frequency of the audio, in Hertz (Hz)\n",
    "    audio - the full audio to compute the spectrogram of; either coldplay or killers\n",
    "    \n",
    "    Output:\n",
    "    Returns a scipy spectrogram for the given audio (in decibels).\n",
    "    \n",
    "    See:\n",
    "    scipy.signal.spectrogram\n",
    "    numpy.log10\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograder.test_Q1c(compute_spectrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look! Run the cell below to compute spectrograms for both *Viva La Vida* and *Mr. Brightside*, then plot their spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spectrogram for Viva La Vida\n",
    "coldplay_spect = compute_spectrogram(fs, coldplay)\n",
    "\n",
    "# Compute spectrogram for Mr. Brightside\n",
    "killers_spect = compute_spectrogram(fs, killers)\n",
    "\n",
    "plt.figure(figsize=(20, 10), dpi=200)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.pcolormesh(t1, f1, coldplay_spect, cmap=\"jet\", shading=\"auto\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "plt.xlabel(\"Time [sec]\")\n",
    "plt.title(\"Viva La Vida\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.pcolormesh(t2, f2, killers_spect, cmap=\"jet\", shading=\"auto\")\n",
    "plt.ylabel(\"Frequency [Hz]\")\n",
    "plt.xlabel(\"Time [sec]\")\n",
    "plt.title(\"Mr. Brightside\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** In both spectrograms, we see a column of dark blue for the first second or so. Based on our colorbar, it looks like this corresponds to $\\approx -300 \\text{dB}$, or essentially no signal power. In terms of the songs, why do we have this in our plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** At the beginning of the spectrogram for Mr. Brightside (after the column of dark blue), you should see two peaks that extend up toward $20 \\text{ kHz}$. What sound in the song is this part of the spectrogram capturing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Can you easily tell the two songs' spectrograms apart? Do you think they'd make good building blocks for our audio recognition algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Fingerprinting\n",
    "\n",
    "Our end goal here is to take an audio snippet and figure out what song's being played. To do this, we'll need a database of songs to compare against. \n",
    "\n",
    "Should we just store entire songs in the database? Probably not, as that'd be a very large database: a three-minute WAV file sampled at $48 \\text{ kHz}$ is a about $30 \\text{ MB}$ in size. Even if we aimed for the modest goal of 1000 songs (which the original iPod from 2001 could hold), we're already looking at using over $30 \\text{ GB}$ of storage. Additionally, comparing raw audio samples for similarity isn't very robust against noise.\n",
    "\n",
    "Instead, we'll generate a set of *fingerprints* from each song, and store these in our database. When our version of Shazam gets fed a song to classify, it can just compare the fingerprints, rather than looking at the whole song. This should solve our storage issues, provided the fingerprints aren't too large. But clearly we'll need this fingerprinting algorithm to have a few other properties for this audio recognition system to be useful.\n",
    "\n",
    "In particular, we want our audio fingerprint to have four key properties:\n",
    "1. ***Temporal Locality:*** We're trying to figure out what song is being played based on a short (say, 5 to 10 second long) clip. So, our fingerprints should somehow encode *where* in the song they come from.\n",
    "\n",
    "2. ***Translational Invariance:*** The snippet we play for Shazam could come from anywhere in the song. We could play it the first 5 seconds, the last 5, or something in the middle. In all cases, we want a correct result, so the same chunk of audio should get the same fingerpint regardless of whether it shows up a minute into a clip or right at the beginning—it's the actual music in it that we should use to generate the fingerprint.\n",
    "\n",
    "3. ***Robustness:*** An audio file, whether clean or degraded by (a modest amount of) noise, should produce the same fingerprint.\n",
    "\n",
    "4. ***High Entropy:*** The fingerprinting algorithm should be \"random enough\" that two different songs don't produce the same fingerprint.\n",
    "\n",
    "As it turns out, spectrograms have all these nice properties, which is why they're such an important part of Shazam! The company's founders recognized this too, and discussed it in their original paper, linked in the references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, spectrograms are cool, but how can we use them? They contain thousands of points... how do we pick which are the most important?**\n",
    "\n",
    "As you might guess, we'll look at the spectrogram's *peaks*: points in high-energy areas. These are the most likely to survive distortions from noise, unlike ones that are close to zero and easily drowned out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2a: Peak Finding\n",
    "\n",
    "To extract these peaks, we want to find areas of the spectrogram where's there's some point that has more energy than its neighbors. To do this, we're going to need some non-linear filtering. \n",
    "\n",
    "### Max Filtering \n",
    "\n",
    "So far in this course, we have almost entirely worked with LTI systems as they're amenable to analysis. However, LTI filtering can't help us here because a \"maximum\" operation is fundamentally non-linear, failing to satisfy the superposition property. \n",
    "\n",
    "To do our peak finding, we'll use Scipy's [`maximum_filter`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.maximum_filter.html) function with a neighborhood of 51 (the `size` parameter in the function call). \n",
    "\n",
    "For each point in our spectrogram, this filter will take our spectrogram $f(x, y)$ and output $g(x, y)$, the maximum value in a 51x51 region around the pixel. \n",
    "\n",
    "Formally,\n",
    "\n",
    "$$g(x, y) = \\max_{i,j} f(x+i, y+j) \\text{  where } -25\\le i, j \\le 25.$$\n",
    "\n",
    "### Your Job\n",
    "\n",
    "Implement the maximum filter and apply it to the spectrogram of \"Viva La Vida.\" When the neighborhood exceeds the boundary of the image, assume $f(x, y)$ is the value of the image at that point (i.e., set `mode='constant'`).\n",
    "\n",
    "\n",
    "After applying the maximum filter:\n",
    "1. Extract a boolean mask which is True when $f(x, y) = g(x, y)$, and False otherwise.\n",
    "2. To ensure these peaks are big enough, in the mask, set any peak locations with a peak less than or equal to `AMP_THRESH` to zero. You can easily, but are not required to, accomplish this with a bitwise `&` operation.\n",
    "3. Use [`np.nonzero`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.nonzero.html) to convert your mask into a set of (frequency, time) pairs. This function will return two arrays. The first is the indices along the frequency axis of the spectrogram where the peaks show up, and the second is the peak indices along the time axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEIGHBORHOOD_SIZE = 2 * 25 + 1\n",
    "AMP_THRESH = 40\n",
    "\n",
    "def peak_finding(spect, neighborhood_size, amp_thresh):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    spect - the spectrogram of the \n",
    "    neighborhood_size - neighborhodo size for the maximum filter\n",
    "    amp_thresh - amplitude threshold to include peaks in result\n",
    "    \n",
    "    Output:\n",
    "    Returns a tuple of the peak index on the frequency \n",
    "    and time axes for the provided spectrograph.\n",
    "    \n",
    "    See:\n",
    "    scipy.ndimage.maximum_filter\n",
    "    np.nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply a Maximum Filter\n",
    "    max_spect = # TODO YOUR CODE HERE\n",
    "\n",
    "    # Compute the mask\n",
    "    mask = # TODO YOUR CODE HERE\n",
    "\n",
    "    # Filter out tiny peaks\n",
    "    # TODO YOUR CODE HERE\n",
    "\n",
    "    # Get the indices of the peaks\n",
    "    freq_idx, time_idx = # TODO YOUR CODE HERE\n",
    "\n",
    "    return freq_idx, time_idx\n",
    "\n",
    "# Call peak_finding with the spectrogram for Viva La Vida\n",
    "freq_idx, time_idx = # TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the next cell and see where our peaks are! We'll label them with black dots for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6), dpi=200)\n",
    "plt.scatter(t1[time_idx], f1[freq_idx], zorder=99, color='k')\n",
    "plt.pcolormesh(t1, f1, coldplay_spect, zorder=0, cmap=\"jet\", shading=\"auto\")\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.title(\"Spectrogram Peaks (for Viva La Vida)\")\n",
    "plt.xlim([0, 60])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** In Q1, we saw how most of the information in music signals is in the lower frequencies (under, say, $10 \\text{ kHz}$). How does this compare with the spectrogram peaks? Are they mostly in lower or upper half of the spectrgram? Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2b: Hashing\n",
    "\n",
    "The peaks we've found make up what the creators of Shazam call a *constellation map*. We'll use the points in our constellation map to compute the song's fingerprints. \n",
    "\n",
    "To do this, we'll take each peak, say $(t_i, f_i)$, and chain it together with the next 15 peaks $(t_{i+1}, f_{i+1}), ..., (t_{i+15}, f_{i+15})$ by:\n",
    "1. Subtracting the times where the peaks show up, computing $t_d = t_{i+j} - t_i$.\n",
    "2. Hash the string $[f_i:f_{i+j}:t_d]$. The fingerprint is $(\\text{hash}, t_i)$.\n",
    "\n",
    "Hashing is out of the scope of this course, so we've provided the hashing function for you (`generate_hash`). At a high level, hashing is a technique that transforms any given key or string into a (essentially) unique hash, or fingerprint. `generate_hash` takes arguments $f_i, f_{i+j}, t_i, t_{i+j}$ (in that order), and does steps 1 and 2, returning the fingerprint, $(\\text{hash}, t_i)$.\n",
    "\n",
    "We've provided `sorted_peaks`, a list of all the peaks sorted in increasing order by the time at which they occur in the song. Note that each element of `sorted_peaks` contains two elements, the $(f, t)$ tuple indicating where the peak occured on the spectrogram. That is, `sorted_peaks[i]` contains the tuple $(f_i, t_i)$.\n",
    "\n",
    "### Your Job\n",
    "\n",
    "Using this, your job is to, for each peak in `sorted_peaks`:\n",
    "1. Extract $(f_i, t_i)$.\n",
    "2. Iterate over the next `HASHES_PER_PEAK` peaks (if you've hit the end of the array, then stop chaining the current peak and move onto the next peak), and:\n",
    "    - Extract $(f_{i+j}, t_{i+j})$.\n",
    "    - Generate the hash using the `generate_hash` function with $f_i, f_{i+j}, t_i, t_{i+j}$.\n",
    "    - Append it to `hashes`.\n",
    "\n",
    "Remember, we only found the indices of the peaks, not the actual times and frequencies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHES_PER_PEAK = 15\n",
    "\n",
    "def hashing(t1, f1, time_idx, freq_idx, hashes_per_peak):\n",
    "    times, freqs = t1[time_idx], f1[freq_idx]\n",
    "    sorted_peaks = sorted(zip(freqs, times), key=lambda x: x[1])\n",
    "    hashes = []\n",
    "\n",
    "    # TODO populate \"hashes\" with the fingerprints\n",
    "\n",
    "    return hashes\n",
    "\n",
    "hashes = hashing(t1, f1, time_idx, freq_idx, HASHES_PER_PEAK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograder.test_Q2b(hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2c: Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can easily compute the fingerprint of a signal! After fingerprinting, all we need to do is search our database for a match. If we did things correctly, the database entry we have the most fingerprints in common with should match the true song.\n",
    "\n",
    "Let's move all of this code into a single function so we can easily compute hashes for any audio signal.\n",
    "\n",
    "`fingerprint` should return an array of tuples, each one containing the hash $h$ and the time $t_i$.\n",
    "\n",
    "**Important notes:**\n",
    "1. **Don't convert the spectrogram to decibels in this part!** We converted the spectrogram to decibels in earlier parts for ease of rendering, but there's no need to do that here (and converting to decibels will cause you to fail the tests).\n",
    "2. **Don't cast the return value to a numpy array.** Each tuple produced by `generate_hash` consists of a string and a number, so numpy ends up turning everything into a string. This will cause tests to fail and cause issues in future parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fingerprint(audio, fs, min_distance=25, amp_thresh=40, hashes_per_peak=15):\n",
    "    NEIGHBORHOOD_SIZE = 2 * min_distance + 1\n",
    "    AMP_THRESH = amp_thresh\n",
    "    HASHES_PER_PEAK = hashes_per_peak\n",
    "    \n",
    "    audio = np.mean(audio, axis=1)\n",
    "    \n",
    "    # TODO: Compute the spectrogram of the single channel audio (Copy from Q1c) \n",
    "    f1, t1, spect = ...\n",
    "\n",
    "    # TODO: Find the peaks (Use function from Q2a)\n",
    "    freq_idx, time_idx = ...\n",
    "    \n",
    "    # TODO: Compute the hashes (Use function from Q2b) and return answer\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograder.test_Q2c(fingerprint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Testing\n",
    "As mentioned before, all we need to do now is test our system and make sure it's as robust as we think it is. Our database is stored in `database.csv`. It's columns are |Hash|t1|Song|. A production application with thousands of songs in the database would use SQL or some other querying language, but a simple CSV will suffice for our uses.\n",
    "\n",
    "Because searching through our database is more of a software problem than a Signals and Systems problem, we've provided the detection function for you. \n",
    "\n",
    "This function:\n",
    "1. Loads the CSV using pandas (a data analysis package),\n",
    "2. Fingerprints the unknown sample,\n",
    "3. Searches for matches, and\n",
    "4. Returns the song with the most matches, its confidence as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect(audio, fs):\n",
    "    db = pd.read_csv(\"database.csv\", header=None, names=[\"Hash\", \"time\", \"Song\"])\n",
    "\n",
    "    hashes = fingerprint(audio, fs)\n",
    "    db_matches = db[db.Hash.isin(map(lambda x: x[0], hashes))]\n",
    "    if len(db_matches) == 0:\n",
    "        print(\"No Matches\")\n",
    "        return\n",
    "\n",
    "    counts = db_matches.groupby(\"Song\").size()\n",
    "    counts = counts / counts.sum()\n",
    "    return counts.idxmax(), counts.max() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3a: Segmenting Audio\n",
    "\n",
    "Shazam usually only has a few seconds of data to work with, so we will as well. Start by writing a function to take a 20 second segment from either Viva La Vida or Mr. Brightside. The specific start and end times don't matter too much, just remember both audio tracks are only 60 seconds long!\n",
    "\n",
    "*Hint:* You've probably seen this operation performed several times already in this lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_20_second_segment(fs, audio):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    fs - the sampling frequency of the audio, in Hertz (Hz)\n",
    "    audio - the full audio to get 20 seconds of; either coldplay or killers\n",
    "    \n",
    "    Output:\n",
    "    A 20 second segment anywhere within the given audio track.\n",
    "    \n",
    "    Example:\n",
    "    get_20_second_segment(killers) == killers[X seconds:(X + 20 seconds)]\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograder.test_Q3a(get_20_second_segment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now use this function to write tests for your Shazam system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3a: Basic Testing\n",
    "\n",
    "Let's see how our system does under ideal conditions (i.e, no noise). Take a 20 second segment from Viva La Vida and Mr. Brightside and call the `detect` function to identify it. We've already reloaded the audio for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, coldplay = wavfile.read(\"VivaLaVida.wav\")\n",
    "fs, killers = wavfile.read(\"MrBrightside.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_detect_test(fs, audio):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    fs - the sampling frequency of the audio, in Hertz (Hz)\n",
    "    audio - the full audio to detect against; either coldplay or killers\n",
    "    \n",
    "    Output:\n",
    "    Returns the name of the audio track that most closely matches \n",
    "    a 20 second segment of the provided audio track, and a percentage confidence.\n",
    "    \n",
    "    Example:\n",
    "    basic_detect_test(killers_fs, killers) == ('MrBrightside.wav', 100.0)\n",
    "    \n",
    "    See also:\n",
    "    detect\n",
    "    get_20_second_segment\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograder.test_Q3b(basic_detect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3c: Gaussian Noise\n",
    "We want our system to be robust to different forms of noise. To start with, lets add some Gaussian noise to our audio and try to detect its origin. Take a 20 second chunk of Viva La Vida, add Gaussian noise with a mean and variance of 10000, and see if you can identify them. \n",
    "\n",
    "*Hint 1*: Checkout the [`np.random.normal`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) function. \n",
    "<br>\n",
    "*Hint 2*: Make sure to pass the `size` parameter to `np.random.normal`\n",
    "<br><br>\n",
    "*Note*: the tests for this section will add random noise to both songs and check that the `detect` function still classifies them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_MEAN = 10000\n",
    "NOISE_VARIANCE = 10000\n",
    "\n",
    "def add_gaussian_noise(audio_segment):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    audio_segment - an audio segment from an unknown track\n",
    "    \n",
    "    Output:\n",
    "    Returns the audio segment with added Gaussian noise.\n",
    "    \n",
    "    See:\n",
    "    Problem description (for quantities)\n",
    "    np.random.normal\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_noise_detect_test(audio_segment):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    audio_segment - an audio segment from an unknown track WITHOUT Gaussian noise\n",
    "    \n",
    "    Output:\n",
    "    Returns the name of the audio track that most closely matches \n",
    "    a 20 second segment of the provided audio track, WITH added \n",
    "    Gaussian noise and a percentage confidence.\n",
    "    \n",
    "    See:\n",
    "    add_gaussian_noise\n",
    "    detect\n",
    "    get_20_second_segment\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograder.test_Q3c(gaussian_noise_detect_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our version of Shazam should still be able to detect the song. How does it sound, though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(add_gaussian_noise(get_20_second_segment(fs, coldplay)).T, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sounds terrible, and we can barely make out the music! Yet, our system still correctly identified it as *Viva La Vida*!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3d: Blocked Speaker\n",
    "\n",
    "What if instead of Gaussian noise, a portion of the audio just becomes zero? Arguably, this is a more realistic model of how our signal could get corrupted when dealing with music recognition. For example, somebody could move in front of the speaker, pause the music, or turn the volume down very low. \n",
    "\n",
    "Let's take a 20 second chunk of Viva La Vida, zero out five 2 second chunks, and see if we can still detect the source.\n",
    "\n",
    "You don't need to implement any code here — just run the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_segment = coldplay[10 * fs: 30 * fs].copy()\n",
    "unknown_segment[:2 * fs] = 0\n",
    "unknown_segment[6 * fs:8 * fs] = 0\n",
    "unknown_segment[16 * fs:20 * fs] = 0\n",
    "unknown_segment[2 * fs:4 * fs] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hear how the song sounds with these portions removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment.T, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Shazam do now? Surely it'll fail with half the clip missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(unknown_segment, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it succeeds! Our fingerprinting procedure is again proving its robustness. What about *Mr. Brightside*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_segment = killers[10 * fs: 30 * fs].copy()\n",
    "unknown_segment[:2 * fs] = 0\n",
    "unknown_segment[6 * fs:8 * fs] = 0\n",
    "unknown_segment[16 * fs:20 * fs] = 0\n",
    "unknown_segment[2 * fs:4 * fs] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(unknown_segment.T, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect(unknown_segment, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our system is pretty robust!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** What happens in the frequency domain when we zero out parts of the signal in the time domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">**A:** (TODO)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='feedback'></a>\n",
    "## Feedback\n",
    "If you have any feedback to give the teaching staff about the course (lab content, staff, etc), you can submit it through this Google form. Responses are **fully anonymous** and responses are actively monitored to improve the labs and course. Completing this form is **not required**.\n",
    "\n",
    "[Anyonymous feedback Google form](https://docs.google.com/forms/d/e/1FAIpQLSdSbJHYZpZqcIKYTw8CfpfrX6OYaGzqlgBtKfsNKEOs4BzZJg/viewform?usp=sf_link)\n",
    "\n",
    "*If you have a personal matter to discuss or need a response to your feedback, please contact <a href=\"mailto:eecs16a.lab@berkeley.edu\">eecs16a.lab@berkeley.edu</a> and/or <a href=\"mailto:eecs16a@berkeley.edu\">eecs16a@berkeley.edu</a>*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='checkoff'></a>\n",
    "## Checkoff\n",
    "To receive credit, all labs will require the submission of a checkoff Google form. This link will be at the bottom of each lab. Both partners should fill out the form (you should have one submission per person), and feel free to use the same Google account/computer to fill it out as long as you have the correct names and student IDs.\n",
    "\n",
    "[Fill out the checkoff Google form.](https://docs.google.com/forms/d/e/1FAIpQLSfIOjvEJXew-M0-h9uJ3C25UOdmmABFK0GGNl3o9p7po7Cc0A/viewform?usp=sf_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comments (Optional)\n",
    "There are many ways to improve our Shazam system. Many of them have to do with how we compute our spectrogram as well as the various parameters we introduced such as `NEIGHBORHOOD_SIZE`, `AMP_THRESH`, and `HASHES_PER_PEAK`. But, for the most part, this is how Shazam works!\n",
    "\n",
    "The original Shazam paper uses a different method for matching the fingerprints of audio instead of a simple \"most matches => song\" scheme, but for our limited database, this works just fine. Check out the original paper if you are curious. If you'd like, you can use the following cells to load your own songs into the database (as long as they are wav files) and try to identify samples of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def add_to_db(filename):\n",
    "    fs, audio = wavfile.read(filename)\n",
    "    hashes = fingerprint(audio, fs)\n",
    "    with open('database.csv', mode='a') as db_file:\n",
    "        db_writer = csv.writer(db_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        for hash_pair in hashes:\n",
    "            db_writer.writerow([hash_pair[0], hash_pair[1], filename])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to add a song to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wav_filepath = ___ # path to any WAV file you want to add to the database\n",
    "add_to_db(my_wav_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] *An industrial strength audio search algorithm.* [[Link](http://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf)].  \n",
    "[2] *Audio fingerprinting with Python and Numpy.* [[Link](https://willdrevo.com/fingerprinting-and-audio-recognition-with-python/)]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
